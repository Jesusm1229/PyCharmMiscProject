{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Weight and Bias Dimensions in Neural Networks\n",
        "\n",
        "## Question\n",
        "\n",
        "**\"Which of the following statements are True? (Check all that apply)\"**\n",
        "\n",
        "Given a neural network with:\n",
        "- 2 input features (x₁, x₂)\n",
        "- 4 neurons in the first hidden layer\n",
        "- 1 neuron in the output layer\n",
        "\n",
        "This notebook explains how to determine the dimensions of W^[1], W^[2], b^[1], and b^[2].\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## General Formulas for Weights and Biases\n",
        "\n",
        "### Key Rules:\n",
        "\n",
        "For any layer `l`:\n",
        "- **W^[l]** (weights) has shape: **(n[l], n[l-1])**\n",
        "  - First dimension: Number of neurons in layer `l`\n",
        "  - Second dimension: Number of neurons in previous layer (or input features for layer 1)\n",
        "  \n",
        "- **b^[l]** (biases) has shape: **(n[l], 1)**\n",
        "  - First dimension: Number of neurons in layer `l`\n",
        "  - Second dimension: Always 1 (one bias per neuron)\n",
        "\n",
        "### Notation:\n",
        "- `n_x` or `n[0]` = number of input features\n",
        "- `n[1]` = number of neurons in layer 1\n",
        "- `n[2]` = number of neurons in layer 2\n",
        "- etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"WEIGHT AND BIAS DIMENSIONS EXPLANATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Network architecture from the question\n",
        "n_x = 2  # Number of input features (x1, x2)\n",
        "n_1 = 4  # Number of neurons in hidden layer 1\n",
        "n_2 = 1  # Number of neurons in output layer\n",
        "\n",
        "print(f\"\\nNetwork Architecture:\")\n",
        "print(f\"  Input features: {n_x}\")\n",
        "print(f\"  Hidden layer 1 neurons: {n_1}\")\n",
        "print(f\"  Output layer neurons: {n_2}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"CALCULATING DIMENSIONS\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dimensions for Layer 1 (Input → Hidden Layer)\n",
        "\n",
        "### W^[1] Dimensions:\n",
        "- Formula: W^[1] shape = (n[1], n[0]) = (n[1], n_x)\n",
        "- Calculation: (4, 2)\n",
        "- **Answer: W^[1] has shape (4, 2)** ✅\n",
        "\n",
        "**Why?**\n",
        "- 4 rows: One row for each neuron in layer 1\n",
        "- 2 columns: One column for each input feature\n",
        "\n",
        "### b^[1] Dimensions:\n",
        "- Formula: b^[1] shape = (n[1], 1)\n",
        "- Calculation: (4, 1)\n",
        "- **Answer: b^[1] has shape (4, 1)** ✅\n",
        "\n",
        "**Why?**\n",
        "- 4 rows: One bias for each neuron in layer 1\n",
        "- 1 column: Always 1 (one bias per neuron)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dimensions for Layer 2 (Hidden Layer → Output Layer)\n",
        "\n",
        "### W^[2] Dimensions:\n",
        "- Formula: W^[2] shape = (n[2], n[1])\n",
        "- Calculation: (1, 4)\n",
        "- **Answer: W^[2] has shape (1, 4)** ✅\n",
        "\n",
        "**Why?**\n",
        "- 1 row: One row for the single neuron in layer 2\n",
        "- 4 columns: One column for each neuron in layer 1\n",
        "\n",
        "### b^[2] Dimensions:\n",
        "- Formula: b^[2] shape = (n[2], 1)\n",
        "- Calculation: (1, 1)\n",
        "- **Answer: b^[2] has shape (1, 1)** ✅\n",
        "\n",
        "**Why?**\n",
        "- 1 row: One bias for the single neuron in layer 2\n",
        "- 1 column: Always 1 (one bias per neuron)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create example arrays to demonstrate dimensions\n",
        "print(\"\\nCreating example arrays:\")\n",
        "\n",
        "# Layer 1\n",
        "W1 = np.random.randn(n_1, n_x)  # (4, 2)\n",
        "b1 = np.random.randn(n_1, 1)    # (4, 1)\n",
        "\n",
        "print(f\"\\nLayer 1:\")\n",
        "print(f\"  W^[1] shape: {W1.shape} = (n[1], n_x) = ({n_1}, {n_x})\")\n",
        "print(f\"  b^[1] shape: {b1.shape} = (n[1], 1) = ({n_1}, 1)\")\n",
        "\n",
        "# Layer 2\n",
        "W2 = np.random.randn(n_2, n_1)  # (1, 4)\n",
        "b2 = np.random.randn(n_2, 1)    # (1, 1)\n",
        "\n",
        "print(f\"\\nLayer 2:\")\n",
        "print(f\"  W^[2] shape: {W2.shape} = (n[2], n[1]) = ({n_2}, {n_1})\")\n",
        "print(f\"  b^[2] shape: {b2.shape} = (n[2], 1) = ({n_2}, 1)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"✅ CORRECT ANSWERS:\")\n",
        "print(\"=\" * 70)\n",
        "print(\"  ✓ W^[1] has shape (4, 2)\")\n",
        "print(\"  ✓ W^[2] has shape (1, 4)\")\n",
        "print(\"  ✓ b^[1] has shape (4, 1)\")\n",
        "print(\"  ✓ b^[2] has shape (1, 1)\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why These Dimensions Work\n",
        "\n",
        "Let's verify the dimensions work correctly in matrix multiplication:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate forward pass with correct dimensions\n",
        "m = 3  # Number of training examples\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"VERIFYING DIMENSIONS WITH FORWARD PASS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Input X: (n_x, m) = (2, 3)\n",
        "X = np.random.randn(n_x, m)\n",
        "print(f\"\\nX (input) shape: {X.shape} = (n_x, m) = ({n_x}, {m})\")\n",
        "\n",
        "# Layer 1 forward pass\n",
        "print(f\"\\nLayer 1 Forward Pass:\")\n",
        "print(f\"  W^[1] shape: ({n_1}, {n_x}) = ({n_1}, {n_x})\")\n",
        "print(f\"  X shape: ({n_x}, {m})\")\n",
        "print(f\"  b^[1] shape: ({n_1}, 1) = ({n_1}, 1)\")\n",
        "\n",
        "# Z[1] = W[1] @ X + b[1]\n",
        "# (4, 2) @ (2, 3) + (4, 1) = (4, 3) + (4, 1) = (4, 3) [broadcasting]\n",
        "Z1 = W1 @ X + b1\n",
        "print(f\"\\n  Z[1] = W^[1] @ X + b^[1]\")\n",
        "print(f\"  Z[1] shape: {Z1.shape} = (n[1], m) = ({n_1}, {m}) ✓\")\n",
        "\n",
        "# Layer 2 forward pass\n",
        "A1 = np.maximum(0, Z1)  # ReLU activation (doesn't change shape)\n",
        "print(f\"\\nLayer 2 Forward Pass:\")\n",
        "print(f\"  A[1] shape: {A1.shape} = (n[1], m) = ({n_1}, {m})\")\n",
        "print(f\"  W^[2] shape: ({n_2}, {n_1}) = ({n_2}, {n_1})\")\n",
        "print(f\"  b^[2] shape: ({n_2}, 1) = ({n_2}, 1)\")\n",
        "\n",
        "# Z[2] = W[2] @ A[1] + b[2]\n",
        "# (1, 4) @ (4, 3) + (1, 1) = (1, 3) + (1, 1) = (1, 3) [broadcasting]\n",
        "Z2 = W2 @ A1 + b2\n",
        "print(f\"\\n  Z[2] = W^[2] @ A[1] + b^[2]\")\n",
        "print(f\"  Z[2] shape: {Z2.shape} = (n[2], m) = ({n_2}, {m}) ✓\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"All dimensions are correct! The matrix multiplications work perfectly.\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why the Other Options are Wrong\n",
        "\n",
        "### ❌ W^[1] has shape (2, 4) - Wrong\n",
        "- This would be the transpose\n",
        "- Would require (2, 4) @ (2, m) which doesn't work\n",
        "- Correct is (4, 2) @ (2, m) = (4, m) ✓\n",
        "\n",
        "### ❌ W^[2] has shape (4, 1) - Wrong\n",
        "- This would be the transpose\n",
        "- Would require (4, 1) @ (4, m) which doesn't work\n",
        "- Correct is (1, 4) @ (4, m) = (1, m) ✓\n",
        "\n",
        "### ❌ b^[1] has shape (2, 1) - Wrong\n",
        "- This would be for 2 neurons, but layer 1 has 4 neurons\n",
        "- Need one bias per neuron → (4, 1) ✓\n",
        "\n",
        "### ❌ b^[2] has shape (4, 1) - Wrong\n",
        "- This would be for 4 neurons, but layer 2 has 1 neuron\n",
        "- Need one bias per neuron → (1, 1) ✓\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary Table\n",
        "\n",
        "| Variable | Formula | This Network | Shape |\n",
        "|----------|---------|--------------|-------|\n",
        "| **W^[1]** | (n[1], n_x) | (4, 2) | (4, 2) ✅ |\n",
        "| **b^[1]** | (n[1], 1) | (4, 1) | (4, 1) ✅ |\n",
        "| **W^[2]** | (n[2], n[1]) | (1, 4) | (1, 4) ✅ |\n",
        "| **b^[2]** | (n[2], 1) | (1, 1) | (1, 1) ✅ |\n",
        "\n",
        "### Memory Trick:\n",
        "- **Weights W^[l]**: (neurons in layer l, neurons in previous layer)\n",
        "- **Biases b^[l]**: (neurons in layer l, 1)\n",
        "\n",
        "Always think: \"How many neurons in this layer?\" → That's the first dimension!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visual summary\n",
        "print(\"=\" * 70)\n",
        "print(\"FINAL ANSWER SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\"\"\n",
        "Network Structure:\n",
        "\n",
        "Input Layer          Hidden Layer 1        Output Layer\n",
        "    x₁ ──────────────┐\n",
        "                      ├──→ a₁^[1] ────────┐\n",
        "    x₂ ──────────────┤   a₂^[1]           │\n",
        "                      ├──→ a₃^[1] ────────┤\n",
        "                      │   a₄^[1]          │\n",
        "                      └───────────────────┼──→ a₁^[2] → ŷ\n",
        "                                         │\n",
        "                                         └───\n",
        "\n",
        "Dimensions:\n",
        "  Input:  X:    (2, m)  ← 2 features, m examples\n",
        "  \n",
        "  Layer 1:\n",
        "    W^[1]: (4, 2)  ← 4 neurons, 2 inputs\n",
        "    b^[1]: (4, 1)  ← 4 neurons, 1 bias each\n",
        "    Z[1]:  (4, m)  ← 4 neurons, m examples\n",
        "    A[1]:  (4, m)  ← 4 neurons, m examples\n",
        "  \n",
        "  Layer 2:\n",
        "    W^[2]: (1, 4)  ← 1 neuron, 4 inputs\n",
        "    b^[2]: (1, 1)  ← 1 neuron, 1 bias\n",
        "    Z[2]:  (1, m)  ← 1 neuron, m examples\n",
        "    A[2]:  (1, m)  ← 1 neuron, m examples\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"CORRECT ANSWERS TO CHECK:\")\n",
        "print(\"=\" * 70)\n",
        "print(\"  ✓ W^[1] will have shape (4, 2)\")\n",
        "print(\"  ✓ W^[2] will have shape (1, 4)\")\n",
        "print(\"  ✓ b^[1] will have shape (4, 1)\")\n",
        "print(\"  ✓ b^[2] will have shape (1, 1)\")\n",
        "print(\"=\" * 70)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

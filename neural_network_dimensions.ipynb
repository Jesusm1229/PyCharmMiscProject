{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neural Network Dimensions: Z[1] and A[1]\n",
        "\n",
        "## Question\n",
        "\n",
        "**\"What are the dimensions of Z[1] and A[1]?\"**\n",
        "\n",
        "Given a neural network with:\n",
        "- 4 input features (x₁, x₂, x₃, x₄)\n",
        "- 2 neurons in the first hidden layer\n",
        "- 1 output neuron\n",
        "\n",
        "This notebook explains how to determine the dimensions of Z[1] and A[1].\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding the Notation\n",
        "\n",
        "### Key Terms:\n",
        "- **Z[l]**: Pre-activation values (weighted sum + bias) for layer `l`\n",
        "- **A[l]**: Post-activation values (output after activation function) for layer `l`\n",
        "- **n[l]**: Number of neurons in layer `l`\n",
        "- **m**: Number of training examples\n",
        "\n",
        "### General Rule:\n",
        "The dimensions of Z[l] and A[l] are:\n",
        "```\n",
        "(n[l], m)\n",
        "```\n",
        "Where:\n",
        "- First dimension: Number of neurons in layer `l`\n",
        "- Second dimension: Number of training examples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"NEURAL NETWORK DIMENSIONS EXPLANATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Network architecture\n",
        "n_x = 4  # Number of input features\n",
        "n_1 = 2  # Number of neurons in layer 1 (hidden layer)\n",
        "n_2 = 1  # Number of neurons in layer 2 (output layer)\n",
        "m = 5    # Number of training examples (example value)\n",
        "\n",
        "print(f\"\\nNetwork Architecture:\")\n",
        "print(f\"  Input features: {n_x}\")\n",
        "print(f\"  Hidden layer 1 neurons: {n_1}\")\n",
        "print(f\"  Output layer neurons: {n_2}\")\n",
        "print(f\"  Training examples: {m}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 70)\n",
        "print(\"DIMENSIONS FOR LAYER 1:\")\n",
        "print(\"-\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dimensions of Z[1] and A[1]\n",
        "\n",
        "### For Layer 1 (First Hidden Layer):\n",
        "\n",
        "**Z[1] dimensions:**\n",
        "- Z[1] contains the pre-activation values for each neuron in layer 1\n",
        "- Number of rows = number of neurons in layer 1 = **2**\n",
        "- Number of columns = number of training examples = **m**\n",
        "\n",
        "**A[1] dimensions:**\n",
        "- A[1] contains the post-activation values for each neuron in layer 1\n",
        "- Same dimensions as Z[1] because activation function doesn't change shape\n",
        "- Number of rows = number of neurons in layer 1 = **2**\n",
        "- Number of columns = number of training examples = **m**\n",
        "\n",
        "**Answer: Z[1] and A[1] are (2, m)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate with actual arrays\n",
        "print(\"\\nCreating example arrays to demonstrate dimensions:\")\n",
        "\n",
        "# Input X: (n_x, m) = (4, m)\n",
        "X = np.random.randn(n_x, m)\n",
        "print(f\"\\nX (input) shape: {X.shape} = (n_x, m) = ({n_x}, {m})\")\n",
        "\n",
        "# Weights W[1]: (n_1, n_x) = (2, 4)\n",
        "W1 = np.random.randn(n_1, n_x)\n",
        "print(f\"W[1] shape: {W1.shape} = (n_1, n_x) = ({n_1}, {n_x})\")\n",
        "\n",
        "# Bias b[1]: (n_1, 1) = (2, 1)\n",
        "b1 = np.random.randn(n_1, 1)\n",
        "print(f\"b[1] shape: {b1.shape} = (n_1, 1) = ({n_1}, 1)\")\n",
        "\n",
        "# Compute Z[1] = W[1] @ X + b[1]\n",
        "# (2, 4) @ (4, 5) + (2, 1) = (2, 5)\n",
        "Z1 = W1 @ X + b1\n",
        "print(f\"\\nZ[1] = W[1] @ X + b[1]\")\n",
        "print(f\"Z[1] shape: {Z1.shape} = (n_1, m) = ({n_1}, {m})\")\n",
        "\n",
        "# Apply activation function (e.g., ReLU)\n",
        "A1 = np.maximum(0, Z1)  # ReLU activation\n",
        "print(f\"\\nA[1] = activation(Z[1])\")\n",
        "print(f\"A[1] shape: {A1.shape} = (n_1, m) = ({n_1}, {m})\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"✅ ANSWER: Z[1] and A[1] are (2, m)\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why Not the Other Options?\n",
        "\n",
        "### ❌ (4, m) - Wrong\n",
        "- This would be the shape of X (input), not Z[1] or A[1]\n",
        "- 4 is the number of input features, not neurons in layer 1\n",
        "\n",
        "### ❌ (4, 1) - Wrong\n",
        "- This would be for a single training example\n",
        "- Missing the `m` dimension for multiple training examples\n",
        "\n",
        "### ❌ (2, 1) - Wrong\n",
        "- This would be for a single training example\n",
        "- Missing the `m` dimension for multiple training examples\n",
        "\n",
        "### ✅ (2, m) - Correct\n",
        "- 2 = number of neurons in layer 1\n",
        "- m = number of training examples\n",
        "- This is the correct shape for both Z[1] and A[1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Matrix Multiplication Dimensions\n",
        "\n",
        "Let's trace through the dimensions:\n",
        "\n",
        "```\n",
        "Forward Pass for Layer 1:\n",
        "\n",
        "X:      (n_x, m)  = (4, m)\n",
        "W[1]:   (n_1, n_x) = (2, 4)\n",
        "b[1]:   (n_1, 1)   = (2, 1)\n",
        "\n",
        "Z[1] = W[1] @ X + b[1]\n",
        "      (2, 4) @ (4, m) + (2, 1)\n",
        "      = (2, m) + (2, 1)  [broadcasting]\n",
        "      = (2, m)\n",
        "\n",
        "A[1] = activation(Z[1])\n",
        "      = activation((2, m))\n",
        "      = (2, m)  [activation doesn't change shape]\n",
        "```\n",
        "\n",
        "**Key insight:** The number of columns (m) comes from the input X, and the number of rows (2) comes from the number of neurons in layer 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visual representation\n",
        "print(\"=\" * 70)\n",
        "print(\"VISUAL REPRESENTATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\"\"\n",
        "Network Structure:\n",
        "\n",
        "Input Layer          Hidden Layer 1        Output Layer\n",
        "    x₁ ──────────────┐\n",
        "                      ├──→ a₁^[1] ────────┐\n",
        "    x₂ ──────────────┤                    ├──→ a₁^[2] → ŷ\n",
        "                      ├──→ a₂^[1] ────────┘\n",
        "    x₃ ──────────────┤\n",
        "                      │\n",
        "    x₄ ──────────────┘\n",
        "\n",
        "Dimensions:\n",
        "  X:    (4, m)  ← 4 features, m examples\n",
        "  Z[1]: (2, m)  ← 2 neurons, m examples\n",
        "  A[1]: (2, m)  ← 2 neurons, m examples\n",
        "  Z[2]: (1, m)  ← 1 neuron, m examples\n",
        "  A[2]: (1, m)  ← 1 neuron, m examples\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\"\"\n",
        "For any layer l:\n",
        "  - Z[l] shape: (n[l], m)\n",
        "  - A[l] shape: (n[l], m)\n",
        "  \n",
        "Where:\n",
        "  - n[l] = number of neurons in layer l\n",
        "  - m = number of training examples\n",
        "\n",
        "For layer 1 in this network:\n",
        "  - n[1] = 2 (2 neurons in hidden layer)\n",
        "  - m = m (number of training examples)\n",
        "  \n",
        "Therefore: Z[1] and A[1] are (2, m)\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## General Formula for All Layers\n",
        "\n",
        "For any layer `l` in a neural network:\n",
        "\n",
        "| Variable | Dimensions | Description |\n",
        "|----------|------------|-------------|\n",
        "| **X** | (n₀, m) | Input features |\n",
        "| **W[l]** | (n[l], n[l-1]) | Weights for layer l |\n",
        "| **b[l]** | (n[l], 1) | Bias for layer l |\n",
        "| **Z[l]** | (n[l], m) | Pre-activation for layer l |\n",
        "| **A[l]** | (n[l], m) | Post-activation for layer l |\n",
        "\n",
        "Where:\n",
        "- `n₀` = number of input features\n",
        "- `n[l]` = number of neurons in layer l\n",
        "- `m` = number of training examples\n",
        "\n",
        "**Remember:** Z[l] and A[l] always have the same shape: **(n[l], m)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complete example showing all layers\n",
        "print(\"=\" * 70)\n",
        "print(\"COMPLETE NETWORK DIMENSIONS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Layer 0 (Input)\n",
        "print(f\"\\nLayer 0 (Input):\")\n",
        "print(f\"  X shape: ({n_x}, m) = ({n_x}, {m})\")\n",
        "\n",
        "# Layer 1 (Hidden)\n",
        "print(f\"\\nLayer 1 (Hidden):\")\n",
        "print(f\"  W[1] shape: ({n_1}, {n_x}) = ({n_1}, {n_x})\")\n",
        "print(f\"  b[1] shape: ({n_1}, 1) = ({n_1}, 1)\")\n",
        "print(f\"  Z[1] shape: ({n_1}, m) = ({n_1}, {m})\")\n",
        "print(f\"  A[1] shape: ({n_1}, m) = ({n_1}, {m})\")\n",
        "\n",
        "# Layer 2 (Output)\n",
        "print(f\"\\nLayer 2 (Output):\")\n",
        "print(f\"  W[2] shape: ({n_2}, {n_1}) = ({n_2}, {n_1})\")\n",
        "print(f\"  b[2] shape: ({n_2}, 1) = ({n_2}, 1)\")\n",
        "print(f\"  Z[2] shape: ({n_2}, m) = ({n_2}, {m})\")\n",
        "print(f\"  A[2] shape: ({n_2}, m) = ({n_2}, {m})\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"FINAL ANSWER TO THE QUESTION:\")\n",
        "print(\"=\" * 70)\n",
        "print(\"Z[1] and A[1] are (2, m)\")\n",
        "print(\"=\" * 70)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

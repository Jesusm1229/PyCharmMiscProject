{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Zero Initialization in Neural Networks\n",
        "\n",
        "## Question\n",
        "\n",
        "**\"Suppose you have built a neural network. You decide to initialize the weights and biases to be zero. Which of the following statements is true?\"**\n",
        "\n",
        "This notebook explains why zero initialization causes a symmetry problem in neural networks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Problem with Zero Initialization\n",
        "\n",
        "### What Happens When We Initialize to Zero?\n",
        "\n",
        "When all weights and biases are initialized to zero:\n",
        "- All neurons in the same layer start with **identical parameters**\n",
        "- They receive **identical inputs** (since previous layer outputs are the same)\n",
        "- They compute **identical outputs**\n",
        "- They receive **identical gradients** during backpropagation\n",
        "- They update in **identical ways**\n",
        "\n",
        "**Result: They remain identical forever!** This is called the **symmetry problem**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Simulate what happens with zero initialization\n",
        "print(\"=\" * 70)\n",
        "print(\"DEMONSTRATION: Zero Initialization Problem\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Initialize weights and biases to zero\n",
        "W1 = np.zeros((3, 2))  # 3 neurons in first hidden layer, 2 inputs\n",
        "b1 = np.zeros((3, 1))  # 3 biases\n",
        "\n",
        "print(\"\\nInitial Weights (W1):\")\n",
        "print(W1)\n",
        "print(\"\\nInitial Biases (b1):\")\n",
        "print(b1)\n",
        "\n",
        "# Simulate forward pass with some input\n",
        "x = np.array([[1.0], [2.0]])  # 2 input features\n",
        "print(f\"\\nInput (x):\\n{x}\")\n",
        "\n",
        "# Compute activations\n",
        "z1 = W1 @ x + b1\n",
        "print(f\"\\nWeighted sum (z1 = W1 @ x + b1):\\n{z1}\")\n",
        "print(\"→ All neurons compute the SAME value (0)!\")\n",
        "\n",
        "# Simulate gradients (simplified - in reality these come from backprop)\n",
        "# But since all neurons are identical, they get identical gradients\n",
        "gradient = np.array([[0.5], [0.5], [0.5]])  # Same gradient for all neurons\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Update weights\n",
        "W1_updated = W1 - learning_rate * gradient @ x.T\n",
        "b1_updated = b1 - learning_rate * gradient\n",
        "\n",
        "print(f\"\\nAfter one gradient descent step:\")\n",
        "print(f\"Updated W1:\\n{W1_updated}\")\n",
        "print(f\"Updated b1:\\n{b1_updated}\")\n",
        "print(\"\\n→ All neurons still have IDENTICAL weights and biases!\")\n",
        "print(\"→ They will compute the SAME thing in the next iteration too!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why This Happens: Mathematical Explanation\n",
        "\n",
        "### Forward Pass\n",
        "If all weights are zero:\n",
        "```\n",
        "Neuron 1: z₁ = 0·x₁ + 0·x₂ + 0 = 0\n",
        "Neuron 2: z₂ = 0·x₁ + 0·x₂ + 0 = 0\n",
        "Neuron 3: z₃ = 0·x₁ + 0·x₂ + 0 = 0\n",
        "```\n",
        "All neurons compute the same value!\n",
        "\n",
        "### Backward Pass (Gradient Descent)\n",
        "Since all neurons have:\n",
        "- Same inputs\n",
        "- Same outputs\n",
        "- Same activations\n",
        "\n",
        "They receive:\n",
        "- **Same gradients** from the loss function\n",
        "- **Same weight updates**\n",
        "\n",
        "After updating:\n",
        "```\n",
        "W₁_new = W₁_old - α·gradient = 0 - α·gradient = -α·gradient\n",
        "W₂_new = W₂_old - α·gradient = 0 - α·gradient = -α·gradient\n",
        "W₃_new = W₃_old - α·gradient = 0 - α·gradient = -α·gradient\n",
        "```\n",
        "\n",
        "**All weights remain identical!** The symmetry is never broken.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visual demonstration: Multiple iterations\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"SIMULATING MULTIPLE ITERATIONS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Reset to zero\n",
        "W = np.zeros((3, 2))\n",
        "b = np.zeros((3, 1))\n",
        "x = np.array([[1.0], [2.0]])\n",
        "\n",
        "print(\"\\nIteration 0 (Initial):\")\n",
        "print(f\"W = \\n{W}\")\n",
        "z = W @ x + b\n",
        "print(f\"Outputs: {z.flatten()}\")\n",
        "\n",
        "# Simulate 5 iterations\n",
        "for i in range(5):\n",
        "    # All neurons get the same gradient (because they're identical)\n",
        "    gradient = np.array([[0.1], [0.1], [0.1]])  # Same for all\n",
        "    W = W - 0.01 * gradient @ x.T\n",
        "    b = b - 0.01 * gradient\n",
        "    z = W @ x + b\n",
        "    \n",
        "    print(f\"\\nIteration {i+1}:\")\n",
        "    print(f\"W = \\n{W}\")\n",
        "    print(f\"Outputs: {z.flatten()}\")\n",
        "    print(f\"→ All neurons still identical!\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"CONCLUSION: Even after multiple iterations, neurons remain identical!\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison: Zero vs Random Initialization\n",
        "\n",
        "Let's see what happens with proper (random) initialization:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"COMPARISON: Zero vs Random Initialization\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Zero initialization\n",
        "W_zero = np.zeros((3, 2))\n",
        "b_zero = np.zeros((3, 1))\n",
        "\n",
        "# Random initialization (proper way)\n",
        "np.random.seed(42)\n",
        "W_random = np.random.randn(3, 2) * 0.01  # Small random values\n",
        "b_random = np.zeros((3, 1))  # Biases can be zero\n",
        "\n",
        "x = np.array([[1.0], [2.0]])\n",
        "\n",
        "print(\"\\n1. ZERO INITIALIZATION:\")\n",
        "z_zero = W_zero @ x + b_zero\n",
        "print(f\"   Outputs: {z_zero.flatten()}\")\n",
        "print(\"   → All neurons produce the same output!\")\n",
        "\n",
        "print(\"\\n2. RANDOM INITIALIZATION:\")\n",
        "z_random = W_random @ x + b_random\n",
        "print(f\"   Outputs: {z_random.flatten()}\")\n",
        "print(\"   → Each neuron produces a DIFFERENT output!\")\n",
        "print(\"   → This breaks the symmetry!\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"KEY INSIGHT:\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\"\"\n",
        "Random initialization ensures:\n",
        "- Each neuron starts with different weights\n",
        "- Each neuron computes different values\n",
        "- Each neuron receives different gradients\n",
        "- Each neuron learns different features\n",
        "- The network can learn diverse representations!\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "cise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solutions: Proper Initialization Methods\n",
        "\n",
        "### 1. Random Initialization (Xavier/Glorot)\n",
        "```python\n",
        "W = np.random.randn(n_neurons, n_inputs) * np.sqrt(1/n_inputs)\n",
        "```\n",
        "\n",
        "### 2. He Initialization (for ReLU)\n",
        "```python\n",
        "W = np.random.randn(n_neurons, n_inputs) * np.sqrt(2/n_inputs)\n",
        "```\n",
        "\n",
        "### 3. Small Random Values\n",
        "```python\n",
        "W = np.random.randn(n_neurons, n_inputs) * 0.01\n",
        "```\n",
        "\n",
        "**All of these break symmetry by giving each neuron different starting weights!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate proper initialization methods\n",
        "print(\"=\" * 70)\n",
        "print(\"PROPER INITIALIZATION METHODS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "n_neurons = 3\n",
        "n_inputs = 2\n",
        "\n",
        "# Method 1: Small random values\n",
        "W1 = np.random.randn(n_neurons, n_inputs) * 0.01\n",
        "print(\"\\n1. Small Random Values (0.01 scale):\")\n",
        "print(f\"   W = \\n{W1}\")\n",
        "print(f\"   → Each weight is different!\")\n",
        "\n",
        "# Method 2: Xavier/Glorot initialization\n",
        "W2 = np.random.randn(n_neurons, n_inputs) * np.sqrt(1/n_inputs)\n",
        "print(\"\\n2. Xavier/Glorot Initialization:\")\n",
        "print(f\"   W = \\n{W2}\")\n",
        "print(f\"   → Scales based on input size!\")\n",
        "\n",
        "# Method 3: He initialization (for ReLU)\n",
        "W3 = np.random.randn(n_neurons, n_inputs) * np.sqrt(2/n_inputs)\n",
        "print(\"\\n3. He Initialization (for ReLU):\")\n",
        "print(f\"   W = \\n{W3}\")\n",
        "print(f\"   → Optimized for ReLU activations!\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"All these methods ensure neurons start with DIFFERENT weights!\")\n",
        "print(\"This breaks symmetry and allows each neuron to learn different features.\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "1. **Zero initialization causes symmetry problem:**\n",
        "   - All neurons in a layer start identical\n",
        "   - They remain identical after gradient descent\n",
        "   - They compute the same thing → wasted capacity\n",
        "\n",
        "2. **Why symmetry doesn't break:**\n",
        "   - Identical weights → identical outputs\n",
        "   - Identical outputs → identical gradients\n",
        "   - Identical gradients → identical updates\n",
        "   - Cycle continues forever\n",
        "\n",
        "3. **Solution: Random initialization**\n",
        "   - Each neuron starts with different weights\n",
        "   - Breaks symmetry from the start\n",
        "   - Allows neurons to learn diverse features\n",
        "\n",
        "4. **The correct answer:**\n",
        "   - ✅ Option 1: Neurons remain identical even after multiple iterations\n",
        "   - This is why we NEVER initialize weights to zero!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

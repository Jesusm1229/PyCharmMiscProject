{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Activation Functions Tutorial\n",
        "\n",
        "This notebook will teach you about four important activation functions used in neural networks:\n",
        "1. **ReLU (Rectified Linear Unit)**\n",
        "2. **Sigmoid**\n",
        "3. **Tanh (Hyperbolic Tangent)**\n",
        "4. **Leaky ReLU**\n",
        "\n",
        "We'll implement each function, visualize them, and understand their properties.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What are Activation Functions?\n",
        "\n",
        "Activation functions are mathematical functions applied to the output of neurons in neural networks. They introduce **non-linearity** into the network, allowing it to learn complex patterns. Without activation functions, a neural network would just be a linear combination of inputs, no matter how many layers it has.\n",
        "\n",
        "Key properties to consider:\n",
        "- **Range**: What values can the function output?\n",
        "- **Differentiability**: Can we compute gradients for backpropagation?\n",
        "- **Saturation**: Does the function saturate (flatten out) at extreme values?\n",
        "- **Computational efficiency**: How fast is it to compute?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'matplotlib'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Set up plotting style\u001b[39;00m\n\u001b[32m      5\u001b[39m plt.style.use(\u001b[33m'\u001b[39m\u001b[33mseaborn-v0_8\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Create input range\n",
        "x = np.linspace(-5, 5, 1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. ReLU (Rectified Linear Unit)\n",
        "\n",
        "### Formula:\n",
        "$$\\text{ReLU}(x) = \\max(0, x) = \\begin{cases} \n",
        "x & \\text{if } x > 0 \\\\\n",
        "0 & \\text{if } x \\leq 0\n",
        "\\end{cases}$$\n",
        "\n",
        "### Properties:\n",
        "- **Range**: [0, ∞)\n",
        "- **Advantages**: \n",
        "  - Simple and computationally efficient\n",
        "  - Solves the vanishing gradient problem (for positive values)\n",
        "  - Most commonly used in hidden layers\n",
        "- **Disadvantages**: \n",
        "  - \"Dying ReLU\" problem: neurons can become inactive (output 0) and never recover\n",
        "  - Not differentiable at x = 0 (though this is rarely a problem in practice)\n",
        "\n",
        "### Use Cases:\n",
        "- Hidden layers in deep neural networks\n",
        "- Convolutional Neural Networks (CNNs)\n",
        "- Most modern deep learning architectures\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "    \"\"\"ReLU activation function\"\"\"\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Compute ReLU\n",
        "y_relu = relu(x)\n",
        "\n",
        "# Plot\n",
        "axes[0].plot(x, y_relu, 'b-', linewidth=2, label='ReLU')\n",
        "axes[0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
        "axes[0].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
        "axes[0].set_xlabel('Input (x)', fontsize=12)\n",
        "axes[0].set_ylabel('Output f(x)', fontsize=12)\n",
        "axes[0].set_title('ReLU Activation Function', fontsize=14, fontweight='bold')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].legend(fontsize=11)\n",
        "axes[0].set_xlim(-5, 5)\n",
        "axes[0].set_ylim(-1, 5)\n",
        "\n",
        "print(\"ReLU Examples:\")\n",
        "print(f\"  ReLU(-3) = {relu(-3)}\")\n",
        "print(f\"  ReLU(0) = {relu(0)}\")\n",
        "print(f\"  ReLU(2.5) = {relu(2.5)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Sigmoid\n",
        "\n",
        "### Formula:\n",
        "$$\\sigma(x) = \\frac{1}{1 + e^{-x}} = \\frac{e^x}{e^x + 1}$$\n",
        "\n",
        "### Properties:\n",
        "- **Range**: (0, 1)\n",
        "- **Advantages**: \n",
        "  - Smooth and differentiable everywhere\n",
        "  - Outputs are always between 0 and 1, making it interpretable as probabilities\n",
        "  - Historically important (used in early neural networks)\n",
        "- **Disadvantages**: \n",
        "  - **Vanishing gradient problem**: Gradients become very small for extreme values\n",
        "  - Outputs are not zero-centered (always positive)\n",
        "  - Computationally more expensive than ReLU\n",
        "\n",
        "### Use Cases:\n",
        "- Output layer for binary classification (probability output)\n",
        "- When you need probabilities between 0 and 1\n",
        "- Less common in hidden layers now (ReLU is preferred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    \"\"\"Sigmoid activation function\"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Compute Sigmoid\n",
        "y_sigmoid = sigmoid(x)\n",
        "\n",
        "# Plot\n",
        "axes[1].plot(x, y_sigmoid, 'r-', linewidth=2, label='Sigmoid')\n",
        "axes[1].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
        "axes[1].axhline(y=1, color='k', linestyle='--', alpha=0.3)\n",
        "axes[1].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
        "axes[1].set_xlabel('Input (x)', fontsize=12)\n",
        "axes[1].set_ylabel('Output f(x)', fontsize=12)\n",
        "axes[1].set_title('Sigmoid Activation Function', fontsize=14, fontweight='bold')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "axes[1].legend(fontsize=11)\n",
        "axes[1].set_xlim(-5, 5)\n",
        "axes[1].set_ylim(-0.1, 1.1)\n",
        "\n",
        "print(\"Sigmoid Examples:\")\n",
        "print(f\"  Sigmoid(-3) = {sigmoid(-3):.4f}\")\n",
        "print(f\"  Sigmoid(0) = {sigmoid(0):.4f}\")\n",
        "print(f\"  Sigmoid(3) = {sigmoid(3):.4f}\")\n",
        "print(f\"\\nNote: Sigmoid(0) = 0.5 (midpoint)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Tanh (Hyperbolic Tangent)\n",
        "\n",
        "### Formula:\n",
        "$$\\tanh(x) = \\frac{e^{2x} - 1}{e^{2x} + 1} = \\frac{\\sinh(x)}{\\cosh(x)}$$\n",
        "\n",
        "### Properties:\n",
        "- **Range**: (-1, 1)\n",
        "- **Advantages**: \n",
        "  - Zero-centered output (unlike sigmoid)\n",
        "  - Smooth and differentiable\n",
        "  - Stronger gradients than sigmoid in the center region\n",
        "- **Disadvantages**: \n",
        "  - Still suffers from vanishing gradient problem at extremes\n",
        "  - Computationally more expensive than ReLU\n",
        "\n",
        "### Use Cases:\n",
        "- Hidden layers (especially in RNNs/LSTMs)\n",
        "- When you want zero-centered outputs\n",
        "- Less common in modern CNNs (ReLU is preferred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tanh(x):\n",
        "    \"\"\"Tanh activation function\"\"\"\n",
        "    return np.tanh(x)\n",
        "\n",
        "# Compute Tanh\n",
        "y_tanh = tanh(x)\n",
        "\n",
        "# Plot\n",
        "axes[2].plot(x, y_tanh, 'g-', linewidth=2, label='Tanh')\n",
        "axes[2].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
        "axes[2].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
        "axes[2].set_xlabel('Input (x)', fontsize=12)\n",
        "axes[2].set_ylabel('Output f(x)', fontsize=12)\n",
        "axes[2].set_title('Tanh Activation Function', fontsize=14, fontweight='bold')\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "axes[2].legend(fontsize=11)\n",
        "axes[2].set_xlim(-5, 5)\n",
        "axes[2].set_ylim(-1.1, 1.1)\n",
        "\n",
        "print(\"Tanh Examples:\")\n",
        "print(f\"  Tanh(-3) = {tanh(-3):.4f}\")\n",
        "print(f\"  Tanh(0) = {tanh(0):.4f}\")\n",
        "print(f\"  Tanh(3) = {tanh(3):.4f}\")\n",
        "print(f\"\\nNote: Tanh(0) = 0 (zero-centered)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Leaky ReLU\n",
        "\n",
        "### Formula:\n",
        "$$\\text{LeakyReLU}(x) = \\begin{cases} \n",
        "x & \\text{if } x > 0 \\\\\n",
        "\\alpha x & \\text{if } x \\leq 0\n",
        "\\end{cases}$$\n",
        "\n",
        "where $\\alpha$ is a small positive constant (typically 0.01)\n",
        "\n",
        "### Properties:\n",
        "- **Range**: (-∞, ∞)\n",
        "- **Advantages**: \n",
        "  - Solves the \"Dying ReLU\" problem by allowing small negative gradients\n",
        "  - Still computationally efficient\n",
        "  - Prevents neurons from becoming completely inactive\n",
        "- **Disadvantages**: \n",
        "  - Requires tuning the $\\alpha$ parameter (though 0.01 is a common default)\n",
        "  - Not differentiable at x = 0 (rarely an issue in practice)\n",
        "\n",
        "### Use Cases:\n",
        "- Alternative to ReLU when you want to avoid dead neurons\n",
        "- Used in some GAN architectures\n",
        "- When training is unstable with standard ReLU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def leaky_relu(x, alpha=0.01):\n",
        "    \"\"\"Leaky ReLU activation function\"\"\"\n",
        "    return np.where(x > 0, x, alpha * x)\n",
        "\n",
        "# Compute Leaky ReLU\n",
        "y_leaky_relu = leaky_relu(x, alpha=0.01)\n",
        "\n",
        "# Plot\n",
        "axes[3].plot(x, y_leaky_relu, 'm-', linewidth=2, label='Leaky ReLU (α=0.01)')\n",
        "axes[3].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
        "axes[3].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
        "axes[3].set_xlabel('Input (x)', fontsize=12)\n",
        "axes[3].set_ylabel('Output f(x)', fontsize=12)\n",
        "axes[3].set_title('Leaky ReLU Activation Function', fontsize=14, fontweight='bold')\n",
        "axes[3].grid(True, alpha=0.3)\n",
        "axes[3].legend(fontsize=11)\n",
        "axes[3].set_xlim(-5, 5)\n",
        "axes[3].set_ylim(-0.5, 5)\n",
        "\n",
        "print(\"Leaky ReLU Examples (α=0.01):\")\n",
        "print(f\"  LeakyReLU(-3) = {leaky_relu(-3):.4f}\")\n",
        "print(f\"  LeakyReLU(0) = {leaky_relu(0):.4f}\")\n",
        "print(f\"  LeakyReLU(2.5) = {leaky_relu(2.5):.4f}\")\n",
        "print(f\"\\nNote: Negative values are multiplied by α instead of being set to 0\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display all plots together\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison: All Functions Together\n",
        "\n",
        "Let's visualize all four functions on the same plot to compare them directly:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.plot(x, y_relu, 'b-', linewidth=2, label='ReLU', alpha=0.8)\n",
        "plt.plot(x, y_sigmoid, 'r-', linewidth=2, label='Sigmoid', alpha=0.8)\n",
        "plt.plot(x, y_tanh, 'g-', linewidth=2, label='Tanh', alpha=0.8)\n",
        "plt.plot(x, y_leaky_relu, 'm-', linewidth=2, label='Leaky ReLU (α=0.01)', alpha=0.8)\n",
        "\n",
        "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
        "plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
        "plt.xlabel('Input (x)', fontsize=12)\n",
        "plt.ylabel('Output f(x)', fontsize=12)\n",
        "plt.title('Comparison of Activation Functions', fontsize=16, fontweight='bold')\n",
        "plt.legend(fontsize=11, loc='best')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xlim(-5, 5)\n",
        "plt.ylim(-1.5, 5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why Do We Need Derivatives of Activation Functions?\n",
        "\n",
        "### The Backpropagation Algorithm\n",
        "\n",
        "**Derivatives are essential for training neural networks** through a process called **backpropagation** (backward propagation of errors). Here's why:\n",
        "\n",
        "1. **How Neural Networks Learn:**\n",
        "   - During training, the network makes predictions\n",
        "   - It compares predictions to actual values (calculates error/loss)\n",
        "   - It needs to adjust the weights to reduce this error\n",
        "   - **Derivatives tell us which direction to adjust each weight** and by how much\n",
        "\n",
        "2. **The Chain Rule:**\n",
        "   - Neural networks are composed of layers: `Input → Layer1 → Layer2 → ... → Output`\n",
        "   - Each layer applies: `output = activation_function(weighted_sum)`\n",
        "   - To update weights in early layers, we need to propagate the error backward\n",
        "   - This requires computing: `∂Error/∂Weight = ∂Error/∂Output × ∂Output/∂Activation × ∂Activation/∂Input × ∂Input/∂Weight`\n",
        "   - **The `∂Activation/∂Input` part is the derivative of the activation function!**\n",
        "\n",
        "3. **What the Derivative Tells Us:**\n",
        "   - **Large derivative** = Strong signal to update weights (learning happens quickly)\n",
        "   - **Small derivative** = Weak signal (learning is slow)\n",
        "   - **Zero derivative** = No learning (weights don't update) - this is the \"vanishing gradient\" problem\n",
        "\n",
        "4. **Example:**\n",
        "   ```\n",
        "   If ReLU(x) = max(0, x):\n",
        "   - For x > 0: derivative = 1 → strong gradient, weights update normally\n",
        "   - For x ≤ 0: derivative = 0 → no gradient, weights don't update (dead neuron)\n",
        "   \n",
        "   This is why Leaky ReLU helps: it has derivative = α (small but non-zero) for x ≤ 0\n",
        "   ```\n",
        "\n",
        "### Why Do We Need Non-Linear Activation Functions?\n",
        "\n",
        "**Without non-linear functions, neural networks would be useless!** Here's why:\n",
        "\n",
        "1. **The Problem with Linear Functions:**\n",
        "   - If all activation functions were linear (like `f(x) = x`), then:\n",
        "   - `Layer1(x) = W₁x + b₁`\n",
        "   - `Layer2(Layer1(x)) = W₂(W₁x + b₁) + b₂ = (W₂W₁)x + (W₂b₁ + b₂)`\n",
        "   - **No matter how many layers you stack, you still get a linear function!**\n",
        "   - A single layer can do the same job → deep networks become pointless\n",
        "\n",
        "2. **What Non-Linearity Enables:**\n",
        "   - **Complex Decision Boundaries:** Non-linear functions allow networks to learn curved, complex boundaries between classes\n",
        "   - **Feature Hierarchies:** Each layer can learn increasingly abstract features\n",
        "   - **Universal Approximation:** With non-linear activations, neural networks can approximate any continuous function (given enough neurons)\n",
        "\n",
        "3. **Real-World Analogy:**\n",
        "   - **Linear:** Can only draw straight lines to separate data\n",
        "   - **Non-Linear:** Can draw curves, circles, and complex shapes to separate data\n",
        "   - Most real-world problems require complex, non-linear decision boundaries\n",
        "\n",
        "4. **Visual Example:**\n",
        "   ```\n",
        "   Linear: Can separate this?  ❌\n",
        "   [●]  [○]  [●]\n",
        "   [○]  [●]  [○]\n",
        "   \n",
        "   Non-Linear: Can separate this?  ✅\n",
        "   [●]  [○]  [●]\n",
        "   [○]  [●]  [○]\n",
        "   (XOR problem - requires non-linearity!)\n",
        "   ```\n",
        "\n",
        "5. **Why Not Just Use Linear Functions?**\n",
        "   - Linear functions can only model linear relationships\n",
        "   - Real-world data is rarely linear (images, speech, text all have complex patterns)\n",
        "   - You'd need infinite linear layers to approximate non-linear functions (impractical)\n",
        "   - Non-linear activations make each layer more powerful\n",
        "\n",
        "**Summary:**\n",
        "- **Derivatives** → Enable backpropagation (how networks learn)\n",
        "- **Non-linearity** → Enables learning complex patterns (what networks can learn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstration: Why Non-Linearity Matters\n",
        "# Let's show what happens when you stack linear vs non-linear layers\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"DEMONSTRATION: Linear vs Non-Linear Functions\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Simulate a simple 3-layer network\n",
        "x = np.array([1.0, 2.0, 3.0])\n",
        "W1 = np.array([[0.5, 0.3, 0.2], [0.1, 0.4, 0.6]])\n",
        "W2 = np.array([[0.7, 0.5], [0.3, 0.9]])\n",
        "W3 = np.array([[0.2, 0.8]])\n",
        "\n",
        "print(\"\\n1. LINEAR ACTIVATION (f(x) = x):\")\n",
        "print(\"-\" * 60)\n",
        "# Linear: just pass through\n",
        "layer1_linear = W1 @ x  # No activation\n",
        "layer2_linear = W2 @ layer1_linear  # No activation\n",
        "output_linear = W3 @ layer2_linear  # No activation\n",
        "print(f\"Input: {x}\")\n",
        "print(f\"Output: {output_linear[0]:.4f}\")\n",
        "\n",
        "# This is equivalent to a single layer!\n",
        "W_equivalent = W3 @ W2 @ W1\n",
        "output_equivalent = W_equivalent @ x\n",
        "print(f\"\\nEquivalent single layer output: {output_equivalent[0]:.4f}\")\n",
        "print(\"→ Same result! Multiple layers add no value with linear activations!\")\n",
        "\n",
        "print(\"\\n2. NON-LINEAR ACTIVATION (ReLU):\")\n",
        "print(\"-\" * 60)\n",
        "# Non-linear: apply ReLU\n",
        "layer1_nonlinear = relu(W1 @ x)\n",
        "layer2_nonlinear = relu(W2 @ layer1_nonlinear)\n",
        "output_nonlinear = W3 @ layer2_nonlinear\n",
        "print(f\"Input: {x}\")\n",
        "print(f\"Layer 1 (after ReLU): {layer1_nonlinear}\")\n",
        "print(f\"Layer 2 (after ReLU): {layer2_nonlinear}\")\n",
        "print(f\"Output: {output_nonlinear[0]:.4f}\")\n",
        "\n",
        "# This CANNOT be reduced to a single layer!\n",
        "print(\"\\n→ Cannot be reduced to a single layer!\")\n",
        "print(\"→ Each layer learns different non-linear transformations!\")\n",
        "print(\"→ This is why deep networks with non-linear activations are powerful!\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"KEY INSIGHT:\")\n",
        "print(\"Linear layers = Can be collapsed into one layer (useless)\")\n",
        "print(\"Non-linear layers = Each layer adds new capabilities (powerful)\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Derivatives/Gradients\n",
        "\n",
        "The derivative of an activation function is crucial for backpropagation. Let's visualize the derivatives:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Derivatives\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    s = sigmoid(x)\n",
        "    return s * (1 - s)\n",
        "\n",
        "def tanh_derivative(x):\n",
        "    return 1 - np.tanh(x)**2\n",
        "\n",
        "def leaky_relu_derivative(x, alpha=0.01):\n",
        "    return np.where(x > 0, 1, alpha)\n",
        "\n",
        "# Compute derivatives\n",
        "d_relu = relu_derivative(x)\n",
        "d_sigmoid = sigmoid_derivative(x)\n",
        "d_tanh = tanh_derivative(x)\n",
        "d_leaky_relu = leaky_relu_derivative(x)\n",
        "\n",
        "# Plot derivatives\n",
        "axes[0].plot(x, d_relu, 'b-', linewidth=2, label=\"ReLU'\")\n",
        "axes[0].set_title('ReLU Derivative', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Input (x)', fontsize=12)\n",
        "axes[0].set_ylabel(\"f'(x)\", fontsize=12)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].set_xlim(-5, 5)\n",
        "axes[0].set_ylim(-0.1, 1.1)\n",
        "\n",
        "axes[1].plot(x, d_sigmoid, 'r-', linewidth=2, label=\"Sigmoid'\")\n",
        "axes[1].set_title('Sigmoid Derivative', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Input (x)', fontsize=12)\n",
        "axes[1].set_ylabel(\"f'(x)\", fontsize=12)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "axes[1].set_xlim(-5, 5)\n",
        "axes[1].set_ylim(-0.1, 0.3)\n",
        "\n",
        "axes[2].plot(x, d_tanh, 'g-', linewidth=2, label=\"Tanh'\")\n",
        "axes[2].set_title('Tanh Derivative', fontsize=14, fontweight='bold')\n",
        "axes[2].set_xlabel('Input (x)', fontsize=12)\n",
        "axes[2].set_ylabel(\"f'(x)\", fontsize=12)\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "axes[2].set_xlim(-5, 5)\n",
        "axes[2].set_ylim(-0.1, 1.1)\n",
        "\n",
        "axes[3].plot(x, d_leaky_relu, 'm-', linewidth=2, label=\"Leaky ReLU'\")\n",
        "axes[3].set_title('Leaky ReLU Derivative', fontsize=14, fontweight='bold')\n",
        "axes[3].set_xlabel('Input (x)', fontsize=12)\n",
        "axes[3].set_ylabel(\"f'(x)\", fontsize=12)\n",
        "axes[3].grid(True, alpha=0.3)\n",
        "axes[3].set_xlim(-5, 5)\n",
        "axes[3].set_ylim(-0.01, 1.1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Key Observations:\")\n",
        "print(\"1. ReLU: Gradient is 1 for x > 0, 0 for x ≤ 0 (vanishing gradient for negatives)\")\n",
        "print(\"2. Sigmoid: Gradient is largest near x=0, very small at extremes (vanishing gradient problem)\")\n",
        "print(\"3. Tanh: Similar to sigmoid but zero-centered, stronger gradients in center\")\n",
        "print(\"4. Leaky ReLU: Gradient is 1 for x > 0, α for x ≤ 0 (solves dying ReLU problem)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practical Examples\n",
        "\n",
        "Let's see how these functions behave with different input values:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with various inputs\n",
        "test_inputs = np.array([-5, -2, -1, 0, 1, 2, 5])\n",
        "\n",
        "print(\"Input\\tReLU\\t\\tSigmoid\\t\\tTanh\\t\\tLeaky ReLU\")\n",
        "print(\"-\" * 70)\n",
        "for val in test_inputs:\n",
        "    print(f\"{val:5.1f}\\t{relu(val):8.4f}\\t{sigmoid(val):8.4f}\\t{tanh(val):8.4f}\\t{leaky_relu(val):8.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary Table\n",
        "\n",
        "| Function | Range | Zero-Centered | Vanishing Gradient | Common Use Case |\n",
        "|----------|-------|---------------|-------------------|-----------------|\n",
        "| **ReLU** | [0, ∞) | No | Yes (for negatives) | Hidden layers in CNNs/DNNs |\n",
        "| **Sigmoid** | (0, 1) | No | Yes (at extremes) | Output layer (binary classification) |\n",
        "| **Tanh** | (-1, 1) | Yes | Yes (at extremes) | Hidden layers (RNNs/LSTMs) |\n",
        "| **Leaky ReLU** | (-∞, ∞) | No | No | Alternative to ReLU |\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "1. **ReLU** is the most popular choice for hidden layers due to its simplicity and effectiveness.\n",
        "2. **Sigmoid** is best for output layers when you need probability outputs (0 to 1).\n",
        "3. **Tanh** is zero-centered, making it sometimes better than sigmoid for hidden layers.\n",
        "4. **Leaky ReLU** solves the \"dying ReLU\" problem by allowing small negative gradients.\n",
        "\n",
        "## When to Use Which?\n",
        "\n",
        "- **Hidden layers**: ReLU or Leaky ReLU (most common)\n",
        "- **Output layer (binary classification)**: Sigmoid\n",
        "- **Output layer (multi-class classification)**: Softmax (not covered here)\n",
        "- **RNNs/LSTMs**: Tanh or ReLU variants\n",
        "\n",
        "## Experiment!\n",
        "\n",
        "Try modifying the Leaky ReLU alpha parameter or test these functions with your own data!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experiment: Try different alpha values for Leaky ReLU\n",
        "alphas = [0.01, 0.1, 0.3]\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for alpha in alphas:\n",
        "    y = leaky_relu(x, alpha=alpha)\n",
        "    plt.plot(x, y, linewidth=2, label=f'Leaky ReLU (α={alpha})')\n",
        "\n",
        "plt.plot(x, y_relu, 'b--', linewidth=2, label='Standard ReLU', alpha=0.5)\n",
        "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
        "plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
        "plt.xlabel('Input (x)', fontsize=12)\n",
        "plt.ylabel('Output f(x)', fontsize=12)\n",
        "plt.title('Leaky ReLU with Different Alpha Values', fontsize=14, fontweight='bold')\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xlim(-5, 5)\n",
        "plt.ylim(-1.5, 5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Notice how larger alpha values allow more negative output, making it closer to a linear function.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "undefined.undefined.undefined"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
